{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethoscopy - Behavpy to HCTSA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This tutorial will take you though converting a behavpy dataset into a format that can be input into HCTSA, a time series comparative analysis software. Currently the full suite of software only works in Matlab. So this tutorial shows you have to convert the data into .mat file that you can then use in the HCTSA environment. The .mat file will include the time series data per specimen, the labels (id), and keywords per specimen.\n",
    "\n",
    "#### If you want to just stick with python see tutorial 5 which uses HCTSA smaller toolbox catch22, that uses the top 22 most common analytical methods from HCTSA in a python environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the dummy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ethoscopy as etho\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "# we need the scipy function savemat to create a .mat file\n",
    "from scipy.io import savemat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data from the overview tutorial\n",
    "from ethoscopy.misc.get_tutorials import get_tutorial\n",
    "data, metadata = get_tutorial('overview')\n",
    "df = etho.behavpy(data, metadata, check = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choose the variable you want to analyse in HCTSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use x (the position of the fly) for this tutorial\n",
    "var = 'x'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HCTSA needs time series data of the same length so we need to do some data curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ethoscope if loading with sleep_annoation will interpolate data about sleep, this causes NaN values for the normal variables other than 'asleep'\n",
    "# So we need to filter it out\n",
    "df = df[df['is_interpolated'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if any NaN values are left\n",
    "df[df[var].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most basic curation is to pick a specific time period\n",
    "df = df.t_filter(start_time = 24, end_time = 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use interpolate to fill in the missing data points, this can be useful if it's only a few points missing per specimen\n",
    "df = df.interpolate(variable = 'x', step_size = 60, t_column = 't')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also group several rows together by increasing the t diff, here we increase\n",
    "# from 60 to 120, so we find the average of every two rows if there\n",
    "df = df.bin_time(column = var, bin_secs =  120, function = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we've completed our curation or if we just want to remove specimens that don't have enough values,\n",
    "# we can call curate to remove all specimens with too few points still\n",
    "df = df.curate(points = ((24*60*60) / 60)-1) # 24*60*60 = seconds in a day and then divided by are t diff per row. \n",
    "\n",
    "# Note: The interpolate method returns rows 1 shorter than before so you'll need to add a minus 1 if using curate after\n",
    "# Note: If you've called the above bin_time this curate will return an empty dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using x position data the interesting part is how the fly positions itself in relation to the food\n",
    "# However this will be different on the x,y axis for flies on either side of the ethoscope, so lets normalise it\n",
    "# You only need to run this is using the x variable\n",
    "df_r = df.xmv('region_id', list(range(11,21)))\n",
    "df_l = df.xmv('region_id', list(range(1,11)))\n",
    "df_r['x'] = 1 - df_r['x']\n",
    "df = df_l.concat(df_r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalise the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ethoscope data can do with a bit of augmentation to make it perform better in HCTSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets put our data into numpy arrays\n",
    "list_x = df.groupby(df.index, sort = False)['x'].apply(list)\n",
    "arr_x = np.array([np.array(x) for x in list_x])\n",
    "# Here we grab the ids of each for the labels that we'll use later\n",
    "list_id = list_x.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use some or all of these functions to normalise the data between specimens\n",
    "\n",
    "# norm transforms the data to be between 0 and 1\n",
    "def norm(x):\n",
    "    return (x-np.nanmin(x))/(np.nanmax(x)-np.nanmin(x))\n",
    "\n",
    "# Only use this if looking at phi, it changes it be only from 0-90 or horizontal to veritcal as the ethoscope doesn't track direction\n",
    "def norm_phi(x):\n",
    "    return np.where(x > 90, 90 - (x - 90), x)\n",
    "\n",
    "# Smooth out the time series data\n",
    "def moving_average(a, n) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can have a look at this effect through some plots\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = np.random.randint(len(arr_x))\n",
    "ind2 = np.random.randint(len(arr_x))\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(arr_x[ind1], color = 'red')\n",
    "plt.plot(arr_x[ind2], color = 'blue')\n",
    "plt.xlabel('Time (s)', fontsize=12)\n",
    "plt.ylabel(f'{var} value', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function along the axis\n",
    "arr_x = np.apply_along_axis(norm, 1, arr_x)\n",
    "arr_x = np.apply_along_axis(partial(moving_average, n = 5), 1, arr_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(arr_x[ind1], color = 'red')\n",
    "plt.plot(arr_x[ind2], color = 'blue')\n",
    "plt.xlabel('Time (s)', fontsize=12)\n",
    "plt.ylabel(f'{var} value', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create the Time Series, Keywords, and labels for the .mat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the time series data is augmented to fit into the correct format\n",
    "# it's stored as the variable ts_mat\n",
    "ts_mat = np.zeros((len(arr_x),), dtype = object) \n",
    "for i in range(len(arr_x)):\n",
    "    ts_mat[i] = arr_x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HCTSA allows keywords to describe each series, we can grab these from the meta data\n",
    "# Feel free to make your own lists of keywords if you haven't got them in your metadata, just make sure they have the right postions in relation to the ids\n",
    "# Filter by the list_id to make sure we only grab those we retained from the data curation and it's the correct order\n",
    "list_sex = df.meta['sex'].filter(list_id).tolist()\n",
    "list_sle = df.meta['sleep_deprived'].filter(list_id).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the keywords together into a single string to match the HCTSA format\n",
    "list_key = np.array([f'{i},{q}' for i, q in zip(list_sex, list_sle)], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the id/label list into an object tyoe numpy array so it formats properly\n",
    "list_lab = np.array(list_id, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a .mat file\n",
    "savemat('./ts.mat', {'timeSeriesData' : ts_mat, 'labels' : list_lab, 'keywords' : list_key})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64774a8dfb1bd896c6efea99b1b4772a6458a05741a63e1cad6fc82c0bcee224"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
